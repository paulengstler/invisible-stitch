import { Link as Highlight, Text, Code, ListItem, Heading, useColorModeValue, Container, Show, Box, Link, OrderedList, UnorderedList, AspectRatio } from '@chakra-ui/react'
import { ExternalLinkIcon } from '@chakra-ui/icons'
import { Title } from 'components/Header'
/*import { Container } from 'components/Container'*/
import { ArticleHeading } from '@/components/ArticleHeading'
import { DarkModeSwitch } from 'components/DarkModeSwitch'
import { LinksRow } from 'components/LinksRow'
import { Footer } from 'components/Footer'
import React, { useState, useEffect } from 'react';

import { title, abstract, citationId, citationAuthors, authors, citationYear, citationBooktitle, acknowledgements } from 'data'

import 'katex/dist/katex.min.css';
import { InlineMath, BlockMath } from 'react-katex';

import { useColorMode, useColorModePreference } from '@chakra-ui/react'
import { Tabs, TabList, TabPanels, Tab, TabPanel, TabIndicator } from '@chakra-ui/react'

function renderViewLarger(url: string, description?: string, renderLinkOnly: boolean = false) {
  const brandSwitch = useColorModeValue('brand.600', 'brand.500')

  return renderLinkOnly ? (
    <>
    <Link href={url} color={brandSwitch} target="_blank" fontFamily={"IBM Plex Sans"}>View a larger version{description !== undefined ? " of this " + description : ""} <ExternalLinkIcon /></Link>
    </>
  ) : (
    <>
    <Text align="left" pt="0.5rem" fontSize="small" fontFamily={"IBM Plex Sans"}><Link href={url} color={brandSwitch} target="_blank">View a larger version{description !== undefined ? " of this " + description : ""} <ExternalLinkIcon /></Link></Text>
    </>
  )
}

function toggleColorToMatchSystem() {
  const { colorMode, toggleColorMode } = useColorMode()
  const systemColorMode = useColorModePreference()

  if (colorMode !== systemColorMode && systemColorMode !== undefined) {
    toggleColorMode()
  }
}

const Index = () => (
  <Box maxWidth="100%" backgroundColor={useColorModeValue('white', 'gray.700')}>
  <Container w="100%" maxWidth="4xl" backgroundColor={useColorModeValue('white', 'gray.700')} paddingStart={{ base: 'var(--chakra-space-0)', md: 'var(--chakra-space-4)'}} paddingEnd={{ base: 'var(--chakra-space-0)', md: 'var(--chakra-space-4)'}}>
    <Container w="100%" maxWidth="4xl" alignItems="left" pl="1.5rem" pr="1.5rem">

      {toggleColorToMatchSystem()}

      {/* Title */}
      <Title />
      {/* <Authors /> */}
      <Box fontFamily={"IBM Plex Sans"}>{ authors.map((author, idx) => 
          <Box display={'inline'}><Link href={author.url} color={useColorModeValue('brand.600', 'brand.500')} fontWeight="600" target="_blank">{author.name}</Link><span>{idx === authors.length - 1 ? "" : " | "}</span></Box>
        ) }</Box>
      <Text fontFamily={"IBM Plex Sans"}fontSize="sm" color={useColorModeValue('gray.600', 'gray.300')}>Visual Geometry Group, University of Oxford</Text>

      <AspectRatio maxW='720px' ratio={3 / 2}>
        <video autoPlay loop muted style={{clipPath: "inset(1px 1px);", marginTop: "1rem"}} playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/paper_480p.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
        </video>
      </AspectRatio>

      {/* Links */}
      <LinksRow />

      <Box textAlign="justify" lineHeight="1.7rem">

      <ArticleHeading id="dataset" mt="0">Building 3D Scenes With Depth Inpainting</ArticleHeading>
      <Text>
      To hallucinate scenes beyond known regions and lift images generated by 2D-based models into three dimensions, current 3D scene generation methods rely on monocular depth estimation networks.
      For this task, it is crucial to <i>seamlessly</i> integrate the newly hallucinated regions into the existing scene representation.
      Simple global scale-and-shift operations to the predicted depth map, as used by previous methods, might lead to discontinuities between the scene and its hallucinated extension.
      We introduce a <b>depth completion network</b> that is able to smoothly extrapolate the existing scene depth based on an input image.
      </Text>
      <Box mt="1rem">
        <img src={`${process.env.BASE_PATH || ""}images/paper_projection_figure_stacked.jpg`} alt="3D scene generation method figure"/>
        {renderViewLarger(`${process.env.BASE_PATH || ""}images/paper_projection_figure_stacked.png`, "figure")}
        <Text fontSize="small" lineHeight={"1.3rem"}>
          <span style={{fontWeight: "600"}}>Overview of our 3D scene generation method. </span> 
          Starting from an input image <InlineMath>I_0</InlineMath>, we project it to a point cloud based on a depth map predicted by a depth estimation network <InlineMath>g</InlineMath>. To extend the scene, we render it from a new view point and query a generative model <InlineMath>f</InlineMath> to hallucinate beyond the scene's boundary. Now, we condition <InlineMath>g</InlineMath> on the depth of the existing scene and the image of the scene extended by <InlineMath>f</InlineMath> to produce a geometrically consistent depth map to project the hallucinated points. This process may be repeated until a 360-degree scene has been generated.
        </Text>
      </Box>

      <Text pt="1rem">
        The depth completion network learns to inpaint masked depth map regions by being conditioned on an image and the depth of known regions. We use masks that represent typical occlusion patterns generated by view point changes. To retain the model's ability to predict depth if no sparse depth is available, the sparse depth input is occasionally dropped.
      </Text>
      <Box mt="1rem">
        <img src={`${process.env.BASE_PATH || ""}images/paper_training_procedure.jpg`} alt="Training procedure figure"/>
        {renderViewLarger(`${process.env.BASE_PATH || ""}images/paper_training_procedure.png`, "figure")}
        <Text fontSize="small" lineHeight={"1.3rem"}>
          <span style={{fontWeight: "600"}}>Overview of our training procedure. </span> 
          In this compact training scheme, a depth completion network <InlineMath>g</InlineMath> is learned by jointly training depth inpainting as well as depth prediction without a sparse depth input (the ratio being determined by the task probability <InlineMath>p</InlineMath>).A teacher network <InlineMath>g_T</InlineMath> is utilized to generate a pseudo ground-truth depth map <InlineMath>D</InlineMath> for a given image <InlineMath>I</InlineMath>. This depth map is then masked with a random mask <InlineMath>M</InlineMath>, to obtain a sparse depth input <InlineMath>\tilde D</InlineMath>.
        </Text>
      </Box>

      <ArticleHeading>360-Degree Scene Results</ArticleHeading>

      <Tabs isLazy variant='unstyled' position="relative">
        <TabPanels>

          <TabPanel>
          <Box display={{ base: "block", md: "flex" }}>
          <video width="360" height="240" autoPlay loop muted playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_zion_360_rgb.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>

          <video width="360" height="240" autoPlay loop muted style={{marginLeft: "1rem"}} playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_zion_360.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          </Box>
          <Box mt="1rem" fontFamily={"var(--chakra-fonts-mono)"} fontSize={"small"}>
          <p>"a view of Zion National Park"</p>
          </Box>
          </TabPanel>
    
          <TabPanel>
          <Box display={{ base: "block", md: "flex" }}>
          <video width="360" height="240" autoPlay loop muted playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_dog_360_rgb.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>

          <video width="360" height="240" autoPlay loop muted style={{marginLeft: "1rem"}} playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_dog_360.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          </Box>
          <Box mt="1rem" fontFamily={"var(--chakra-fonts-mono)"} fontSize={"small"}>
          <p>"a close-up view of a muddy path in a forest"</p>
          </Box>
          </TabPanel>

          <TabPanel>

          <Box display={{ base: "block", md: "flex" }}>
          <video width="360" height="240" autoPlay loop muted playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_cairo_360_rgb.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>

          <video width="360" height="240" autoPlay loop muted style={{marginLeft: "1rem"}} playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_cairo_360.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          </Box>
          <Box mt="1rem" fontFamily={"var(--chakra-fonts-mono)"} fontSize={"small"}>
          <p>"a view of Cairo, Egypt"</p>
          </Box>

          </TabPanel>

          <TabPanel>
          <Box display={{ base: "block", md: "flex" }}>
          <video width="360" height="240" autoPlay loop muted playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_tent_360_rgb.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>

          <video width="360" height="240" autoPlay loop muted style={{marginLeft: "1rem"}} playsInline>
            <source src={`${process.env.BASE_PATH || ""}videos/demo_tent_360.mp4`} type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          </Box>
          <Box mt="1rem" fontFamily={"var(--chakra-fonts-mono)"} fontSize={"small"}>
          <p>"a warm living room with plants"</p>
          </Box>
          </TabPanel>
        </TabPanels>
        <TabList>
          <Tab><Box backgroundImage={`${process.env.BASE_PATH || ""}images/photo-1469559845082-95b66baaf023.jpeg`} width="64px" height="64px" backgroundSize={"cover"} /></Tab>
          <Tab><Box backgroundImage={`${process.env.BASE_PATH || ""}images/photo-1514984879728-be0aff75a6e8.jpeg`} width="64px" height="64px" backgroundSize={"cover"} /></Tab>
          <Tab><Box backgroundImage={`${process.env.BASE_PATH || ""}images/photo-1572252009286-268acec5ca0a.jpeg`} width="64px" height="64px" backgroundSize={"cover"} /></Tab>
          <Tab><Box backgroundImage={`${process.env.BASE_PATH || ""}images/photo-1546975490-e8b92a360b24.jpeg`} width="64px" height="64px" backgroundSize={"cover"} /></Tab>
        </TabList>
        <TabIndicator height='4px' bg={useColorModeValue('brand.600', 'brand.500')} borderRadius='8px'/>
      </Tabs>

      <ArticleHeading>Evaluating Scene Geometry</ArticleHeading>
      Within the fully generative task of scene generation, evaluating the geometric properties of generated scenes is difficult due to the lack of ground-truth data.
      Most existing work resorts to image-text similarity scores, which only measures the global semantic alignment of the generation with a text description.
      To evaluate the geometric consistency and quality of the depth predictions used to build the scene, we propose a new evaluation benchmark.
      This benchmark quantifies the depth-reconstruction quality on a partial scene with known ground truth depth.

      <Box mt="1rem">
        <img src={`${process.env.BASE_PATH || ""}images/paper_sce_figure.jpg`} alt="Scene evaluation approach figure"/>
        {renderViewLarger(`${process.env.BASE_PATH || ""}images/paper_sce_figure.png`, "figure")}
        <Text fontSize="small" lineHeight={"1.3rem"}>
          <span style={{fontWeight: "600"}}>Overview of our scene consistency evaluation approach. </span> 
          Assume a scene is described by a set of views <InlineMath math="\{v_1, v_2, \dots\}"></InlineMath> with associated images, depth maps, and camera poses, where the overlap of two views is described by a function <InlineMath math="\phi(v_i, v_j)"></InlineMath>. For a given view pair <InlineMath math="(v_i, v_j)"></InlineMath> with <InlineMath math="\phi(v_i, v_j) \geq \tau"></InlineMath>, we generate a representation, e.g., a point cloud, from the ground-truth (GT) data for <InlineMath math="v_i"></InlineMath>. Then, we render the representation from the view point of <InlineMath math="v_j"></InlineMath>. We feed the corresponding ground-truth image and the representation's depth into the model under consideration to extrapolate the missing depth. Finally, we calculate the mean absolute error between the result and the ground-truth depth for <InlineMath math="v_j"></InlineMath>, only considering those regions that were extrapolated.
        </Text>
      </Box>

      <Text pt="1rem">
        In both, a real-world and a photorealistic setting, our inpainting model produces predictions that are more faithful to the ground-truth than the other methods.
      </Text>

      {/* Abstract */}
      <ArticleHeading>Abstract</ArticleHeading>
      <Text>{abstract}</Text>

      </Box>

      {/* Acknowledgements */}
      <Box pb="2rem">
      <ArticleHeading fontSize="md" color={useColorModeValue('gray.600', 'gray.300')}>Acknowledgements</ArticleHeading>
      <Text fontSize="small" color={useColorModeValue('gray.600', 'gray.300')}>
        {acknowledgements}
      </Text>
      </Box>

  </Container>
  </Container>
  </Box>
)

export default Index